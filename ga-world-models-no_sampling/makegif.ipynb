{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "\n",
    "from models import MDRNNCell, VAE, Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = './results/'\n",
    "files = os.listdir(dir_path)\n",
    "files = [f for f in files if 'best_1_1_G' in f]\n",
    "\n",
    "num = -1\n",
    "idx = 0\n",
    "file = ''\n",
    "for idx, fi in enumerate(files):\n",
    "    Gnum = fi.split('G')[1].split('.')[0]\n",
    "    Gnum = int(Gnum)\n",
    "    if Gnum > num:\n",
    "        file = fi\n",
    "        num = Gnum\n",
    "        \n",
    "file_path = dir_path + file\n",
    "s = torch.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ASIZE, LSIZE, RSIZE, RED_SIZE, SIZE = 3, 32, 256, 64, 64\n",
    "\n",
    "vae = VAE(3, LSIZE, 1024) # VAE：3チャンネル、潜在ベクトルサイズ、中間層サイズ\n",
    "mdrnn = MDRNNCell(LSIZE, ASIZE, RSIZE, 5) # MDRNN：潜在ベクトルサイズ、アクションサイズ、中間層サイズ、混合ガウス分布の分布数\n",
    "controller = Controller(LSIZE, RSIZE, ASIZE) # コントローラー：潜在ベクトルサイズ、中間層サイズ、アクションサイズ\n",
    "\n",
    "vae.load_state_dict(s['vae'])\n",
    "mdrnn.load_state_dict(s['mdrnn'])\n",
    "controller.load_state_dict(s['controller'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VAE                                      [1, 3, 64, 64]            --\n",
       "├─Encoder: 1-1                           [1, 32]                   --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 31, 31]           1,568\n",
       "│    └─Conv2d: 2-2                       [1, 64, 14, 14]           32,832\n",
       "│    └─Conv2d: 2-3                       [1, 128, 6, 6]            131,200\n",
       "│    └─Conv2d: 2-4                       [1, 256, 2, 2]            524,544\n",
       "│    └─Linear: 2-5                       [1, 32]                   32,800\n",
       "│    └─Linear: 2-6                       [1, 32]                   32,800\n",
       "├─Decoder: 1-2                           [1, 3, 64, 64]            --\n",
       "│    └─Linear: 2-7                       [1, 1024]                 33,792\n",
       "│    └─ConvTranspose2d: 2-8              [1, 128, 5, 5]            3,276,928\n",
       "│    └─ConvTranspose2d: 2-9              [1, 64, 13, 13]           204,864\n",
       "│    └─ConvTranspose2d: 2-10             [1, 32, 30, 30]           73,760\n",
       "│    └─ConvTranspose2d: 2-11             [1, 3, 64, 64]            3,459\n",
       "==========================================================================================\n",
       "Total params: 4,348,547\n",
       "Trainable params: 4,348,547\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 211.96\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.84\n",
       "Params size (MB): 17.39\n",
       "Estimated Total Size (MB): 18.28\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(vae, (1, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((RED_SIZE, RED_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daiki\\miniconda3\\envs\\env_torch_world_models\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "early_termination = True\n",
    "time_limit = 2000\n",
    "imgs = []\n",
    "\n",
    "def get_action_and_transition(obs, hidden):\n",
    "        \"\"\" 行動を起こし、遷移\n",
    "\n",
    "        VAEを用いて観測値を潜在状態に変換し、MDRNNを用いて次の潜在状態と次の隠れ状態の推定を行い、コントローラに対応するアクションを計算する。\n",
    "\n",
    "        :args obs: current observation (1 x 3 x 64 x 64) torch tensor\n",
    "        :args hidden: current hidden state (1 x 256) torch tensor\n",
    "\n",
    "        :returns: (action, next_hidden)\n",
    "            - action: 1D np array\n",
    "            - next_hidden (1 x 256) torch tensor\n",
    "        \"\"\"\n",
    "        _, latent_mu, _ = vae(obs) # latent_mu: 潜在ベクトルの期待値\n",
    "\n",
    "        action = controller(latent_mu, hidden[0] ) # コントローラーによるアクションの計算\n",
    "\n",
    "        mus, sigmas, logpi, rs, d, next_hidden = mdrnn(action, latent_mu, hidden) # MDRNNによる次の潜在状態と次の隠れ状態の推定\n",
    "\n",
    "        return action.squeeze().cpu().numpy(), next_hidden\n",
    "\n",
    "\n",
    "with torch.no_grad():          \n",
    "    env = gym.make('CarRacing-v2', render_mode='rgb_array', domain_randomize=False) # 環境：CarRacing-v2\n",
    "\n",
    "    obs, _ = env.reset() # 環境のリセット\n",
    "    imgs.append(obs) # 画像の取得\n",
    "\n",
    "    hidden = [\n",
    "        torch.zeros(1, RSIZE)#.to(device) # 隠れ状態の初期化\n",
    "        for _ in range(2)]\n",
    "\n",
    "    neg_count = 0 # 負の報酬を受け取った回数\n",
    "\n",
    "    cumulative = 0 # 累積報酬\n",
    "    i = 0\n",
    "    while True:\n",
    "        obs = transform(obs).unsqueeze(0)#.to(device) # 観測（画像）の前処理：obs(1, 3, 64, 64)\n",
    "        \n",
    "        action, hidden = get_action_and_transition(obs, hidden) # 行動を起こし、遷移：action(1, ASIZE), hidden(1, RSIZE)\n",
    "        #Steering: Real valued in [-1, 1] \n",
    "        #Gas: Real valued in [0, 1]\n",
    "        #Break: Real valued in [0, 1]\n",
    "\n",
    "        obs, reward, done, _, _ = env.step(action) # 行動を実行し、報酬を受け取る：obs(3, 64, 64), reward, done, info\n",
    "        imgs.append(obs) # 画像の取得\n",
    "        \n",
    "        #報酬を得られなかった（コース外に出たなど）連続回数をカウント\n",
    "        neg_count = neg_count+1 if reward < 0.0 else 0 \n",
    "        \n",
    "        #トレーニングのスピードアップのために、コース外の評価を行い，20time step以上コース外に出た場合はロールアウトを終了する\n",
    "        if (neg_count>20 and early_termination):  \n",
    "            done = True\n",
    "        \n",
    "        cumulative += reward # 累積報酬の更新\n",
    "        \n",
    "        # ロールアウトの終了：タイムリミットに達した場合、早期終了した場合, 完了した場合\n",
    "        if done or (early_termination and i > time_limit):\n",
    "            env.close()\n",
    "            break\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902.799999999984"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "imageio.mimsave('./results/rollout.gif', imgs, duration=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch_world_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
